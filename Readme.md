# ğŸ“Œ Research Assistant Job Scraper

## ğŸ“ Project Overview
This project is designed to **scrape job listings** for research assistant and pre-doctoral positions from multiple sources, process the data, and display it in a **web interface** for easy filtering and viewing. It also allows **automatic updates** and **email notifications** when new positions are found.

## ğŸ“‚ Folder Structure
```
â”œâ”€â”€ app.py                   # Flask web app for viewing job listings
â”œâ”€â”€ environment.yml          # Conda environment configuration
â”œâ”€â”€ jobs.xml                 # Stores the latest scraped job listings
â”œâ”€â”€ main.ipynb               # Jupyter notebook for manual testing
â”œâ”€â”€ main.py                  # Main script to scrape jobs and update XML
â”œâ”€â”€ previous_jobs.xml        # Backup of the previous job listings
â”œâ”€â”€ sources                  # Directory for downloaded HTML pages
â”‚   â”œâ”€â”€ ejm.html             # Cached EJM job listings
â”‚   â”œâ”€â”€ nber.html            # Cached NBER job listings
â”‚   â””â”€â”€ predoc.html          # Cached Predoc job listings
â”œâ”€â”€ static                   # Static files for the web app
â”‚   â””â”€â”€ style.css            # CSS for dark mode styling
â””â”€â”€ templates                # HTML templates for Flask
    â””â”€â”€ index.html           # Main page displaying job listings
```

## ğŸš€ How to Set Up the Project
### 1ï¸âƒ£ Install Dependencies
You can create the necessary environment using Conda:
```sh
conda env create -f environment.yml
conda activate ra
```
Alternatively, if you are using `pip`:
```sh
pip install -r requirements.txt
```

### 2ï¸âƒ£ Set Up Environment Variables
To configure email notifications, create a `.env` file in the root folder and add:
```
SENDER_EMAIL=your_email@example.com
SENDER_PASSWORD=your_app_password
```
> âš ï¸ **Important**: For Gmail, you need to generate an **App Password** instead of using your actual password.

## ğŸŒ Web Scraping (Downloading HTML Pages)
### Automated Download (MacOS & Linux)
In **MacOS** and **Linux**, the script includes these two commands to **download the HTML of the Predoc site** and save it in `sources/`:
```sh
!mkdir -p sources
!curl -L "https://predoc.org/opportunities" -o "sources/predoc.html"
```
> **Windows Alternative**: These commands won't work on **Windows**. You should manually open the **Predoc** website, save the **HTML file**, and place it in the `sources/` folder.

## â³ Automating Execution (MacOS/Linux)
To **run `main.py` automatically every day at 16:00**, you can create a **Cron Job**:
```sh
crontab -e
```
Then add the following line:
```sh
0 16 * * * /bin/bash -c 'source ~/miniconda3/etc/profile.d/conda.sh && conda activate ra && python ~/path/to/main.py >> ~/cron_log.txt 2>&1'
```
To **monitor the logs**, run:
```sh
tail -f ~/cron_log.txt
```
> If you are using **Windows**, you should create a **Task Scheduler** job instead.

## ğŸŒ Running the Web App
Once you have scraped job data into `jobs.xml`, you can launch the **Flask web app**:
```sh
python app.py
```
Then open **http://127.0.0.1:5000** in your browser.

## ğŸ¤ Contributing
This is an **open-source project**, and contributions are **welcome!** ğŸš€ If you would like to **improve the web scraping**, **optimize performance**, or **enhance the web interface**, feel free to submit **pull requests** or report issues.

