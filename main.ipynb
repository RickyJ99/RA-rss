{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping to generate RSS feed for new positions in economics\n",
    "This application is using three main sources to retrieve information about the new job posts:\n",
    "1. [NBER](https://www.nber.org/career-resources/research-assistant-positions-not-nber)\n",
    "2. [Predoc](https://predoc.org/opportunities)\n",
    "3. [EconJobMarket](https://econjobmarket.org/market)\n",
    "   \n",
    "The packeges that are needed are **requests**, **beautifulsoup4**,**MIMEtext**. As a first step we recall them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/riccardodalcero/miniconda3/envs/ra/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/riccardodalcero/miniconda3/envs/ra/lib/python3.12/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/riccardodalcero/miniconda3/envs/ra/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/riccardodalcero/miniconda3/envs/ra/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/riccardodalcero/miniconda3/envs/ra/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/riccardodalcero/miniconda3/envs/ra/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import certifi\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from IPython.display import Markdown, display\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv() #just for email and password "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE = \"jobs.csv\" #database saved in simple csv file\n",
    "PREDOC_URL = \"https://predoc.org/opportunities\"\n",
    "NBER_URL = \"https://www.nber.org/career-resources/research-assistant-positions-not-nber\"\n",
    "EJM_URL = \"https://econjobmarket.org/market\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the html\n",
    "The following functions are downloading the HTML content from the sources and it save it in the foulder sources.\n",
    "For PREDOC there is a issue with certificate so it is easy to use curl (bash MacOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  407k  100  407k    0     0   163k      0  0:00:02  0:00:02 --:--:--  163k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p sources\n",
    "!curl -L \"https://predoc.org/opportunities\" -o \"sources/predoc.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading https://predoc.org/opportunities: HTTPSConnectionPool(host='predoc.org', port=443): Max retries exceeded with url: /opportunities (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)')))\n",
      "Downloaded HTML from https://www.nber.org/career-resources/research-assistant-positions-not-nber to sources/nber.html\n",
      "Downloaded HTML from https://econjobmarket.org/market to sources/ejm.html\n"
     ]
    }
   ],
   "source": [
    "def download_html(url, filename):\n",
    "    \"\"\"\n",
    "    Downloads the HTML content from the given URL and saves it to the specified filename.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, verify=certifi.where())\n",
    "        response.raise_for_status()\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"Downloaded HTML from {url} to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "# Ensure the 'sources' folder exists.\n",
    "os.makedirs(\"sources\", exist_ok=True)\n",
    "\n",
    "# Download HTML content for each source.\n",
    "download_html(PREDOC_URL, \"sources/predoc.html\")\n",
    "download_html(NBER_URL, \"sources/nber.html\")\n",
    "download_html(EJM_URL, \"sources/ejm.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Web Scraping Section üöÄ\n",
    "\n",
    "In this section, we set up our web scraping functionality. Our goal is to **extract job details** from pre-doctoral opportunities pages (in this example, from [predoc.org](https://predoc.org/opportunities)). We assume that the HTML content has already been downloaded and saved locally in the `sources` folder.\n",
    "\n",
    "### Predoc\n",
    "What This Section Does:\n",
    "- **Reads the Local HTML File üìÇ:**  \n",
    "  We read the downloaded HTML file (`sources/predoc.html`). If the file isn't found, the code prompts you to download it first.\n",
    "  \n",
    "- **Parses the HTML Content ü•£:**  \n",
    "  Using BeautifulSoup, the code parses the HTML to locate the container that holds the opportunity details.\n",
    "  \n",
    "- **Extracts Key Information üîç:**  \n",
    "  For each job posting, the function extracts:\n",
    "  - **Program Title** and **Link** from the `<h2>` element.\n",
    "  - Additional details (like **sponsor**, **institution**, **fields of research**, and **deadline**) from the \"copy\" `<div>`.\n",
    "  \n",
    "- **Determines the Main Field üîë:**  \n",
    "  It combines several text fields and passes them to an auxiliary function (`extract_main_field()`) that determines the primary focus (e.g., Economics, Microeconomics, Finance, etc.).\n",
    "\n",
    "- **Returns the Data as a List üì§:**  \n",
    "  Each job is stored as a dictionary, and the function returns a list of these dictionaries.\n",
    "\n",
    "> **Note:**  \n",
    "> Make sure to download the HTML file before running the scraper (therefore run the previous chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_predoc():\n",
    "    \"\"\"\n",
    "    Scrapes the pre-doctoral opportunities page from the local HTML file\n",
    "    and extracts job details.\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    \n",
    "    # Attempt to read the local HTML file. üìÇ\n",
    "    try:\n",
    "        with open(\"sources/predoc.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "    except Exception as e:\n",
    "        print(\"Error reading sources/predoc.html. Please download the HTML from predoc before proceeding. üö´\")\n",
    "        return jobs  # Return an empty list if the file can't be read.\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup. ü•£\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Find the container holding the opportunities using a regex on the class name. üîç\n",
    "    container = soup.find(\"div\", class_=re.compile(\"Opportunities\"))\n",
    "    if not container:\n",
    "        print(\"No Predoc container found. üò¢\")\n",
    "        return jobs\n",
    "    \n",
    "    # Loop over each article element within the container. üìù\n",
    "    articles = container.find_all(\"article\")\n",
    "    for article in articles:\n",
    "        job = {}\n",
    "        job[\"source\"] = \"predoc\"  # Mark the source as 'predoc'. üåü\n",
    "        \n",
    "        # Extract the title and link from the <h2> element. üè∑Ô∏è\n",
    "        h2 = article.find(\"h2\")\n",
    "        if h2:\n",
    "            a_tag = h2.find(\"a\")\n",
    "            if a_tag:\n",
    "                job[\"program_title\"] = a_tag.get_text(strip=True)\n",
    "                job[\"link\"] = a_tag.get(\"href\", \"\").strip()\n",
    "            else:\n",
    "                job[\"program_title\"] = \"N/A\"\n",
    "                job[\"link\"] = \"\"\n",
    "        else:\n",
    "            job[\"program_title\"] = \"N/A\"\n",
    "            job[\"link\"] = \"\"\n",
    "        \n",
    "        # Extract details from the \"copy\" div. üóíÔ∏è\n",
    "        copy_div = article.find(\"div\", class_=\"copy\")\n",
    "        if copy_div:\n",
    "            p = copy_div.find(\"p\")\n",
    "            if p:\n",
    "                text = p.get_text(separator=\" \", strip=True)\n",
    "                # Use regex to capture specific fields from the text. üîç\n",
    "                researcher_match = re.search(r\"Sponsoring Researcher\\(s\\):\\s*(.*?)\\s*Sponsoring Institution:\", text)\n",
    "                institution_match = re.search(r\"Sponsoring Institution:\\s*(.*?)\\s*Fields of Research\", text)\n",
    "                fields_match = re.search(r\"Fields of Research\\s*:\\s*(.*?)\\s*Deadline:\", text)\n",
    "                deadline_match = re.search(r\"Deadline:\\s*(.*)\", text)\n",
    "                job[\"sponsor\"] = researcher_match.group(1).strip() if researcher_match else \"N/A\"\n",
    "                job[\"institution\"] = institution_match.group(1).strip() if institution_match else \"N/A\"\n",
    "                job[\"fields\"] = fields_match.group(1).strip() if fields_match else \"N/A\"\n",
    "                job[\"deadline\"] = deadline_match.group(1).strip() if deadline_match else \"N/A\"\n",
    "            else:\n",
    "                job[\"sponsor\"] = \"N/A\"\n",
    "                job[\"institution\"] = \"N/A\"\n",
    "                job[\"fields\"] = \"N/A\"\n",
    "                job[\"deadline\"] = \"N/A\"\n",
    "        else:\n",
    "            job[\"sponsor\"] = \"N/A\"\n",
    "            job[\"institution\"] = \"N/A\"\n",
    "            job[\"fields\"] = \"N/A\"\n",
    "            job[\"deadline\"] = \"N/A\"\n",
    "        \n",
    "        # Add additional fields for consistency. üõ†Ô∏è\n",
    "        job[\"university\"] = \"N/A\"\n",
    "        job[\"program_type\"] = \"N/A\"\n",
    "        job[\"publication_date\"] = \"N/A\"\n",
    "        \n",
    "        # Determine the main field by combining text from various fields. üîë\n",
    "        text_to_search = \" \".join([job.get(\"fields\", \"\"), job.get(\"program_title\", \"\"), job.get(\"institution\", \"\")])\n",
    "        job[\"main_field\"] = extract_main_field(text_to_search)\n",
    "        \n",
    "        # Append the extracted job details to the jobs list. ‚úÖ\n",
    "        jobs.append(job)\n",
    "    \n",
    "    # Return the list of all extracted job details. üì§\n",
    "    return jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Section for NBER (Local HTML) üîé\n",
    "\n",
    "In this section, we extract job details from the locally saved NBER page HTML file. The function follows these steps:\n",
    "\n",
    "- **üìÇ Read the Local HTML File:**  \n",
    "  The function attempts to read `sources/nber.html`. If the file isn't found, it prints an error message and returns an empty list.\n",
    "\n",
    "- **ü•£ Parse HTML with BeautifulSoup:**  \n",
    "  The HTML content is parsed so we can navigate and extract the data.\n",
    "\n",
    "- **üîç Locate the Container:**  \n",
    "  It finds the `<div>` with class `page-header__intro-inner` that holds the job details.\n",
    "\n",
    "- **‚úÇÔ∏è Skip Header Paragraphs:**  \n",
    "  The first three `<p>` elements are skipped as they contain header information.\n",
    "\n",
    "- **üìã Extract Job Details:**  \n",
    "  For each job posting, the function extracts:\n",
    "  - Program title  \n",
    "  - Sponsor  \n",
    "  - Institution  \n",
    "  - Fields of research  \n",
    "  - Job link  \n",
    "  If any of these details are missing, it defaults to `\"N/A\"`.\n",
    "\n",
    "- **üîë Determine Main Field:**  \n",
    "  It combines relevant text and uses the helper function `extract_main_field()` (which should be defined elsewhere) to determine the primary research area.\n",
    "\n",
    "- **‚úÖ Return the Jobs List:**  \n",
    "  Finally, all extracted job entries are stored in a list and returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nber():\n",
    "    \"\"\"\n",
    "    Scrapes the NBER research assistant positions page from a local HTML file\n",
    "    and extracts job details.\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    \n",
    "    # Attempt to read the local HTML file. üìÇ\n",
    "    try:\n",
    "        with open(\"sources/nber.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "    except Exception as e:\n",
    "        print(\"Error reading sources/nber.html. Please download the HTML from NBER before proceeding. üö´\")\n",
    "        return jobs  # Return an empty list if the file can't be read.\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup. ü•£\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Find the container holding the job details using its class name. üîç\n",
    "    container = soup.find(\"div\", class_=\"page-header__intro-inner\")\n",
    "    if container:\n",
    "        # Get all <p> elements inside the container. üìù\n",
    "        paragraphs = container.find_all(\"p\")\n",
    "        # Skip the first three header paragraphs. ‚úÇÔ∏è\n",
    "        for p in paragraphs[3:]:\n",
    "            job = {}\n",
    "            job[\"source\"] = \"nber\"  # Mark the source as NBER. üåü\n",
    "            parts = p.decode_contents().split(\"<br>\")\n",
    "            if len(parts) >= 5:\n",
    "                job[\"program_title\"] = parts[0].strip()\n",
    "                job[\"sponsor\"] = parts[1].replace(\"NBER Sponsoring Researcher(s):\", \"\").strip()\n",
    "                job[\"institution\"] = parts[2].replace(\"Institution:\", \"\").strip()\n",
    "                job[\"fields\"] = parts[3].replace(\"Field(s) of Research:\", \"\").strip()\n",
    "                \n",
    "                # Extract the job link from the HTML in the last part. üîó\n",
    "                link_soup = BeautifulSoup(parts[4], \"html.parser\")\n",
    "                a_tag = link_soup.find(\"a\")\n",
    "                job[\"link\"] = a_tag[\"href\"] if a_tag else \"\"\n",
    "            else:\n",
    "                # Default values if parts are missing. üò¢\n",
    "                job[\"program_title\"] = \"N/A\"\n",
    "                job[\"sponsor\"] = \"N/A\"\n",
    "                job[\"institution\"] = \"N/A\"\n",
    "                job[\"fields\"] = \"N/A\"\n",
    "                job[\"link\"] = \"\"\n",
    "            \n",
    "            job[\"deadline\"] = \"N/A\"  # Deadline not provided. ‚è∞\n",
    "            job[\"university\"] = \"N/A\"\n",
    "            job[\"program_type\"] = \"N/A\"\n",
    "            job[\"publication_date\"] = \"N/A\"\n",
    "            \n",
    "            # Combine text fields to determine the main field using a helper function. üîë\n",
    "            text_to_search = \" \".join([job.get(\"fields\", \"\"), job.get(\"program_title\", \"\"), job.get(\"institution\", \"\")])\n",
    "            job[\"main_field\"] = extract_main_field(text_to_search)\n",
    "            \n",
    "            # Append the extracted job to our list. ‚úÖ\n",
    "            jobs.append(job)\n",
    "    else:\n",
    "        print(\"NBER container not found. üò¢\")\n",
    "    \n",
    "    # Return the list of all extracted job details. üì§\n",
    "    return jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Section for EJM (Econ Job Market) üîé\n",
    "\n",
    "This function is designed to scrape job postings from the Econ Job Market (EJM) page. It performs the following tasks:\n",
    "\n",
    "- **üåê Fetching the Page:**  \n",
    "  It sends an HTTP GET request to the EJM URL using the `requests` library.\n",
    "\n",
    "- **ü•£ Parsing HTML:**  \n",
    "  The response content is parsed with BeautifulSoup to create a DOM structure for extraction.\n",
    "\n",
    "- **üîç Locating Job Panels:**  \n",
    "  It finds all `<div>` elements with the classes `\"panel panel-info\"`, each representing a job posting.\n",
    "\n",
    "- **üè∑Ô∏è Extracting Job Details:**  \n",
    "  For each panel, it extracts:\n",
    "  - **Job Title & Link:** Located within an `<a>` tag with an ID starting with \"title-\".  \n",
    "  - **University & Program Type:** Extracted from `<div>` elements with class `\"col-md-4\"` and `\"col-md-2\"`, respectively.\n",
    "  - **Publication Date & Deadline:** Extracted from `<div>` elements with class `\"col-md-2\"`.\n",
    "  - **Default Values:** Fields such as **sponsor**, **institution**, and **fields** are set to `\"N/A\"` since they're not provided.\n",
    "  \n",
    "- **üîë Determining the Main Field:**  \n",
    "  It combines the program title and university information to deduce the primary research field using the helper function `extract_main_field()`.\n",
    "\n",
    "- **‚úÖ Building the Result List:**  \n",
    "  Each job is stored as a dictionary, and all such dictionaries are appended to a list which is then returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_ejm():\n",
    "    \"\"\"\n",
    "    Scrapes the Econ Job Market (EJM) page and extracts job details.\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    try:\n",
    "        # Send an HTTP GET request to the EJM page. üåê\n",
    "        response = requests.get(EJM_URL)\n",
    "        response.raise_for_status()  # Ensure we got a valid response. ‚úÖ\n",
    "        \n",
    "        # Parse the response content using BeautifulSoup. ü•£\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Find all panels that represent job postings. üîç\n",
    "        panels = soup.find_all(\"div\", class_=\"panel panel-info\")\n",
    "        for panel in panels:\n",
    "            job = {}\n",
    "            job[\"source\"] = \"ejm\"  # Mark the source as EJM. üåü\n",
    "            \n",
    "            # Extract title and link from the <a> element with an ID starting with 'title-'. üè∑Ô∏è\n",
    "            title_a = panel.find(\"a\", id=lambda x: x and x.startswith(\"title-\"))\n",
    "            if title_a:\n",
    "                job[\"program_title\"] = title_a.get_text(strip=True)\n",
    "                job[\"link\"] = title_a.get(\"href\", \"\").strip()\n",
    "            else:\n",
    "                job[\"program_title\"] = \"N/A\"\n",
    "                job[\"link\"] = \"\"\n",
    "            \n",
    "            # Extract university and program type from the designated columns.\n",
    "            cols = panel.find_all(\"div\", class_=\"col-md-4\")\n",
    "            if len(cols) >= 2:\n",
    "                job[\"university\"] = cols[1].get_text(\" \", strip=True)\n",
    "            else:\n",
    "                job[\"university\"] = \"N/A\"\n",
    "            \n",
    "            col_md2 = panel.find(\"div\", class_=\"col-md-2\")\n",
    "            if col_md2:\n",
    "                job[\"program_type\"] = col_md2.get_text(\" \", strip=True)\n",
    "            else:\n",
    "                job[\"program_type\"] = \"N/A\"\n",
    "            \n",
    "            # Extract publication and deadline dates from the columns with class 'col-md-2'.\n",
    "            cols_date = panel.find_all(\"div\", class_=\"col-md-2\")\n",
    "            if cols_date:\n",
    "                if len(cols_date) >= 1:\n",
    "                    job[\"publication_date\"] = cols_date[0].get_text(\" \", strip=True)\n",
    "                else:\n",
    "                    job[\"publication_date\"] = \"N/A\"\n",
    "                if len(cols_date) >= 2:\n",
    "                    job[\"deadline\"] = cols_date[1].get_text(\" \", strip=True)\n",
    "                else:\n",
    "                    job[\"deadline\"] = \"N/A\"\n",
    "            else:\n",
    "                job[\"publication_date\"] = \"N/A\"\n",
    "                job[\"deadline\"] = \"N/A\"\n",
    "            \n",
    "            # EJM pages may not include these details, so we use default values. üò¢\n",
    "            job[\"sponsor\"] = \"N/A\"\n",
    "            job[\"institution\"] = \"N/A\"\n",
    "            job[\"fields\"] = \"N/A\"\n",
    "            \n",
    "            # Combine text fields (program title and university) to determine the main field. üîë\n",
    "            text_to_search = \" \".join([job.get(\"program_title\", \"\"), job.get(\"university\", \"\")])\n",
    "            job[\"main_field\"] = extract_main_field(text_to_search)\n",
    "            \n",
    "            # Append the job details to our list. ‚úÖ\n",
    "            jobs.append(job)\n",
    "    except Exception as e:\n",
    "        print(\"Error during EJM scraping:\", e)\n",
    "    return jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Main Field Helper Function üîë\n",
    "\n",
    "The `extract_main_field` function analyzes a given text to determine which research fields are mentioned. It searches for multiple keywords in a **case-insensitive** manner. If one or more keywords are found, it returns them as a comma‚Äëseparated string. If none are found, it returns `\"N/A\"`.\n",
    "\n",
    "### Keywords Included:\n",
    "- **Economics**\n",
    "- **Macroeconomics**\n",
    "- **Microeconomics**\n",
    "- **Labour**\n",
    "- **Industrial Organization**\n",
    "- **Enterpreneurship**\n",
    "- **Healthcare**\n",
    "- **Discrimination**\n",
    "- **Finance**\n",
    "- **Public Policy**\n",
    "\n",
    "You can extend this list with additional fields in economics as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_field(text):\n",
    "    \"\"\"\n",
    "    Looks for keywords in the provided text.\n",
    "    Keywords: Economics, Macroeconomics, Microeconomics, Labour, Industrial Organization,\n",
    "    Enterpreneurship, Healthcare, Discrimination, Finance, Public Policy.\n",
    "    Returns a comma-separated string of all found keywords or \"N/A\" if none are found.\n",
    "    \"\"\"\n",
    "    keywords = [\n",
    "        \"Economics\", \"Macroeconomics\", \"Microeconomics\",\n",
    "        \"Labour\", \"Industrial Organization\", \"Enterpreneurship\",\n",
    "        \"Healthcare\", \"Discrimination\", \"Finance\", \"Public Policy\"\n",
    "    ]\n",
    "    \n",
    "    found = []\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in text.lower():\n",
    "            found.append(keyword)\n",
    "    \n",
    "    if found:\n",
    "        # Remove duplicates while preserving order and return as a comma-separated string.\n",
    "        unique_keywords = list(dict.fromkeys(found))\n",
    "        return \", \".join(unique_keywords)\n",
    "    else:\n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV & Email Handling Section üìä‚úâÔ∏è\n",
    "\n",
    "This section contains helper functions to manage your job database and send email notifications when new opportunities are detected.\n",
    "\n",
    "### 1. Reading Existing Jobs from a CSV File üìÇ\n",
    "\n",
    "The `read_existing_jobs()` function reads a CSV file that contains saved job listings and returns a **set** of job links that are already recorded.  \n",
    "- It checks if the file exists.  \n",
    "- It uses Python's `csv.DictReader` to iterate over rows and collects the \"link\" field for each job.  \n",
    "\n",
    "### 2. Appending New Jobs to the CSV File üíæ\n",
    "\n",
    "The `append_jobs_to_csv()` function takes a list of job dictionaries and appends them to the specified CSV file.\n",
    "- If the CSV file doesn't exist, it creates the file and writes the header.  \n",
    "- It then appends each job as a new row.  \n",
    "\n",
    "### 3. Sending Email Notifications ‚úâÔ∏è\n",
    "\n",
    "- **Purpose:**  \n",
    "  This function sends an email notification whenever new job records are found.  \n",
    "  - **Single Record:** The subject is set to the university name from that record.\n",
    "  - **Multiple Records:** The subject lists the unique university names (e.g., \"University A, University B\").\n",
    "\n",
    "- **Email Body:**  \n",
    "  The email body is constructed as an HTML document with a styled table that lists:\n",
    "  - **Source** (e.g., \"predoc\", \"nber\", \"ejm\")\n",
    "  - **Program Title**\n",
    "  - **Clickable Link** (each link is rendered as a clickable hyperlink)\n",
    "  - **Sponsor**\n",
    "  - **Institution**\n",
    "  - **Fields**\n",
    "  - **Main Field**\n",
    "  - **Deadline**\n",
    "  - **University**\n",
    "  - **Program Type**\n",
    "  - **Publication Date**\n",
    "\n",
    "- **How It Works:**  \n",
    "  1. **Subject Creation:**  \n",
    "     The function extracts university names from each job record. If there's only one record, it uses that university name; if multiple, it joins all unique names.\n",
    "  \n",
    "  2. **HTML Table Construction:**  \n",
    "     An HTML table is built with one row per job record, ensuring that links are rendered as clickable hyperlinks.\n",
    "  \n",
    "  3. **Email Assembly:**  \n",
    "     The email is composed as a multipart message with both plain text and HTML parts.\n",
    "  \n",
    "  4. **Sending the Email:**  \n",
    "     Using Python's `smtplib`, the function logs in to the SMTP server (defaulting to Gmail) and sends the email.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_existing_jobs(csv_file):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and returns a set of job links that are already recorded.\n",
    "    \"\"\"\n",
    "    existing_links = set()\n",
    "    if os.path.exists(csv_file):\n",
    "        with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                if \"link\" in row:\n",
    "                    existing_links.add(row[\"link\"])\n",
    "    return existing_links\n",
    "\n",
    "def append_jobs_to_csv(csv_file, jobs, fieldnames):\n",
    "    \"\"\"\n",
    "    Appends the list of job dictionaries to the CSV file.\n",
    "    If the file does not exist, it is created and a header is written.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    with open(csv_file, \"a\", newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for job in jobs:\n",
    "            writer.writerow(job)\n",
    "\n",
    "def send_email_new_jobs(new_jobs, sender_email, sender_password, receiver_email, smtp_server=\"smtp.gmail.com\", smtp_port=587):\n",
    "    \"\"\"\n",
    "    Sends an email with new job records.\n",
    "    \n",
    "    If there is a single record, the email subject is set to the university name from that record.\n",
    "    If there are multiple records, the subject lists the unique university names (comma-separated).\n",
    "    \n",
    "    The email body is an HTML table containing one row per job record.\n",
    "    Clickable hyperlinks are created for the job links.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract university names from the new jobs (ignoring \"N/A\")\n",
    "    universities = [job.get(\"university\", \"N/A\") for job in new_jobs if job.get(\"university\", \"N/A\") != \"N/A\"]\n",
    "    unique_universities = list(dict.fromkeys(universities))  # preserve order, remove duplicates\n",
    "    \n",
    "    # Set email subject based on number of records:\n",
    "    if len(new_jobs) == 1:\n",
    "        subject = unique_universities[0] if unique_universities else \"New Research Position\"\n",
    "    else:\n",
    "        subject = \", \".join(unique_universities) if unique_universities else \"New Research Positions\"\n",
    "    \n",
    "    # Construct the HTML table for the email body\n",
    "    html_body = \"\"\"\n",
    "    <html>\n",
    "      <head>\n",
    "        <style>\n",
    "          table, th, td {\n",
    "            border: 1px solid #ddd;\n",
    "            border-collapse: collapse;\n",
    "            padding: 8px;\n",
    "          }\n",
    "          th {\n",
    "            background-color: #f2f2f2;\n",
    "          }\n",
    "        </style>\n",
    "      </head>\n",
    "      <body>\n",
    "        <p>New Research Positions Found:</p>\n",
    "        <table>\n",
    "          <tr>\n",
    "            <th>Source</th>\n",
    "            <th>Program Title</th>\n",
    "            <th>Link</th>\n",
    "            <th>Sponsor</th>\n",
    "            <th>Institution</th>\n",
    "            <th>Fields</th>\n",
    "            <th>Main Field</th>\n",
    "            <th>Deadline</th>\n",
    "            <th>University</th>\n",
    "            <th>Program Type</th>\n",
    "            <th>Publication Date</th>\n",
    "          </tr>\n",
    "    \"\"\"\n",
    "    for job in new_jobs:\n",
    "        link = job.get(\"link\", \"\")\n",
    "        # Make the link clickable if available\n",
    "        clickable_link = f'<a href=\"{link}\">{link}</a>' if link else \"N/A\"\n",
    "        html_body += f\"\"\"\n",
    "          <tr>\n",
    "            <td>{job.get(\"source\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"program_title\", \"N/A\")}</td>\n",
    "            <td>{clickable_link}</td>\n",
    "            <td>{job.get(\"sponsor\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"institution\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"fields\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"main_field\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"deadline\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"university\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"program_type\", \"N/A\")}</td>\n",
    "            <td>{job.get(\"publication_date\", \"N/A\")}</td>\n",
    "          </tr>\n",
    "        \"\"\"\n",
    "    html_body += \"\"\"\n",
    "        </table>\n",
    "      </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a multipart email message (plain text and HTML)\n",
    "    msg = MIMEMultipart(\"alternative\")\n",
    "    msg[\"Subject\"] = subject\n",
    "    msg[\"From\"] = sender_email\n",
    "    msg[\"To\"] = receiver_email\n",
    "    \n",
    "    # Plain text version as fallback\n",
    "    text_body = \"New Research Positions Found. Please view this email in an HTML-compatible client.\"\n",
    "    \n",
    "    part1 = MIMEText(text_body, \"plain\")\n",
    "    part2 = MIMEText(html_body, \"html\")\n",
    "    \n",
    "    msg.attach(part1)\n",
    "    msg.attach(part2)\n",
    "    \n",
    "    # Send the email via SMTP\n",
    "    try:\n",
    "        with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
    "            server.starttls()  # Secure the connection\n",
    "            server.login(sender_email, sender_password)\n",
    "            server.send_message(msg)\n",
    "        print(\"Email sent successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed to send email:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function: Scrape, Update, and Notify üöÄüìä‚úâÔ∏è\n",
    "\n",
    "This **main()** function orchestrates the complete workflow of the project. It:\n",
    "\n",
    "- **Scrapes Job Data:**  \n",
    "  Calls the scraping functions for all three sources (Predoc, NBER, EJM) to collect job postings.\n",
    "\n",
    "- **Filters New Jobs:**  \n",
    "  Reads an existing CSV file (acting as a simple database) to get a set of already recorded job links. Then, it filters out jobs that are already present.\n",
    "\n",
    "- **Sends Notifications:**  \n",
    "  For each new job found, the function sends an email notification with the job details.\n",
    "\n",
    "- **Updates the CSV Database:**  \n",
    "  Finally, it appends the new job entries to the CSV file for future reference.\n",
    "\n",
    "> **Note:**  \n",
    "> Ensure that your SMTP credentials (i.e. `SENDER_EMAIL` and `SENDER_PASSWORD`) are set up and that the scraping functions (`scrape_predoc()`, `scrape_nber()`, and `scrape_ejm()`) along with CSV and email helper functions are defined before running `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Scrape jobs from all three sources. üåê\n",
    "    jobs = []\n",
    "    jobs.extend(scrape_predoc())\n",
    "    jobs.extend(scrape_nber())\n",
    "    jobs.extend(scrape_ejm())\n",
    "\n",
    "    # Check if any jobs were scraped. üö®\n",
    "    if not jobs:\n",
    "        print(\"No jobs were scraped.\")\n",
    "        return\n",
    "\n",
    "    # Read existing jobs from CSV (using 'link' as a unique identifier). üìÇ\n",
    "    existing_links = read_existing_jobs(CSV_FILE)\n",
    "\n",
    "    # Filter out jobs that are already recorded. üîç\n",
    "    new_jobs = [job for job in jobs if job.get(\"link\") not in existing_links]\n",
    "    print(f\"Found {len(new_jobs)} new job(s).\")\n",
    "\n",
    "    # Define CSV columns (fieldnames) for consistent data structure. üìã\n",
    "    fieldnames = [\n",
    "        \"source\", \"program_title\", \"link\", \"sponsor\",\n",
    "        \"institution\", \"fields\", \"main_field\", \"deadline\",\n",
    "        \"university\", \"program_type\", \"publication_date\"\n",
    "    ]\n",
    "\n",
    "    # If new jobs are found, process them. ‚úâÔ∏èüíæ\n",
    "    if new_jobs:\n",
    "        # Retrieve SMTP credentials from environment variables.\n",
    "        sender_email    = os.getenv('SENDER_EMAIL')\n",
    "        sender_password = os.getenv('SENDER_PASSWORD')\n",
    "        receiver_email  = os.getenv('SENDER_EMAIL')\n",
    "        \n",
    "        # Convert the new jobs to a Pandas DataFrame for easy visualization. üìà\n",
    "        df = pd.DataFrame(new_jobs)\n",
    "        md_table = df.to_markdown(index=False)\n",
    "        # Uncomment these lines if you want to send an email and update the CSV.\n",
    "        # send_email_new_jobs(new_jobs, sender_email, sender_password, receiver_email)\n",
    "        # append_jobs_to_csv(CSV_FILE, new_jobs, fieldnames)\n",
    "        \n",
    "        # Print the DataFrame to visualize the new jobs.\n",
    "        \n",
    "    else:\n",
    "        # If no new jobs were found, print a message and display existing links.\n",
    "        print(\"No new jobs found.\")\n",
    "        df = pd.DataFrame(list(existing_links), columns=[\"link\"])\n",
    "        md_table = df.to_markdown(index=False)\n",
    "    # Display the table in the notebook\n",
    "    display(Markdown(md_table))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 new job(s).\n",
      "No new jobs found.\n",
      "                            link\n",
      "0                               \n",
      "1         https://bit.ly/40M5pz6\n",
      "2                      #ad-10962\n",
      "3         https://bit.ly/40MdGDf\n",
      "4         https://bit.ly/40POY4Y\n",
      "..                           ...\n",
      "149       https://bit.ly/47y04yB\n",
      "150       https://bit.ly/42mLf1b\n",
      "151       https://bit.ly/4f2ipGs\n",
      "152       https://bit.ly/3XOeQ0C\n",
      "153  https://stanford.io/4hF4vMG\n",
      "\n",
      "[154 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
